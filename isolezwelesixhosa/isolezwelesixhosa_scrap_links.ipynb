{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "isolezwelesixhosa_scrap_links.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "8gsn_1x_vL2f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib import request\n",
        "from urllib.request import Request, urlopen\n",
        "agent = {\"User-Agent\":'Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/59.0.3071.115 Safari/537.36'}"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9yEs4umAuUmH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#define new category aside from the home page\n",
        "category = ['iindaba','ezemidlalo','ezoyolo','izimvo']\n",
        "#get the number of `featured` pages each of the categories have\n",
        "def get_no_featuredpages(catgory):\n",
        "  url = \"https://www.isolezwelesixhosa.co.za/\"+catgory+\"?filter_by=featured\"\n",
        "  reqs = requests.get(url, headers=agent)\n",
        "  soup = BeautifulSoup(reqs.content, 'lxml')\n",
        "  res = soup.find('span',{'class':'pages'})\n",
        "  return int(res.text.split()[-1])\n",
        "\n",
        "#scrap links from the different categories \n",
        "def scrap_link(catgory, catgory_size):\n",
        "  urls = []\n",
        "  print(\"Getting the URL from the category : \", catgory, \" (featured) with \", catgory_size, \" pages . \" )\n",
        "  for p in range(catgory_size):\n",
        "    url = 'https://www.isolezwelesixhosa.co.za/'+catgory+'/page/'+str(p+1)+'?filter_by=featured'\n",
        "    #print(url)\n",
        "    reqs = requests.get(url, headers=agent)\n",
        "    soup = BeautifulSoup(reqs.content, 'lxml')\n",
        "    res = soup.find_all('div',{'class':'td-module-thumb'})#.find_all(\"a\", href=True)\n",
        "    #print(res)\n",
        "    urls.extend([t.a['href'] for t in res])\n",
        "  #return all the news article links from the category\n",
        "  return urls\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nLJmXou28hZD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#scrap the articles with links in the `urls` variable\n",
        "def getSoup(links):\n",
        "  print(\"getting soup objects for all the links ... \")\n",
        "  soupx = []\n",
        "  cnt=0\n",
        "  for url in links: \n",
        "    page_request = request.Request(url, headers=agent)\n",
        "    page = request.urlopen(page_request)\n",
        "    #response = requests.get(page_url)\n",
        "    #print(page.getcode())\n",
        "    soup = BeautifulSoup(page, 'html.parser')\n",
        "    soupx.append(soup)\n",
        "    cnt = cnt + 1\n",
        "    progresse = (cnt/len(links) * 100) \n",
        "    if progresse  % 10 == 0:\n",
        "      print (\"Got \", progresse, \"% of soup objects\")\n",
        "  print(\"got the soup object for all the links ... \")\n",
        "  return soupx\n",
        "\n",
        "def getcontent(soupx):\n",
        "  print(\"scrapping the articles ... \")\n",
        "  cnt=0;\n",
        "  title=[];time=[]; texts=[]\n",
        "  for soups in soupx:\n",
        "    cnt=cnt+1\n",
        "    if soups.find(\"h1\", {\"class\":\"entry-title\"})!= None:\n",
        "      title.append(soups.find(\"h1\", {\"class\":\"entry-title\"}).text.replace(\"\\t\",\"\").replace(\"\\n\",\"\"))\n",
        "    else:\n",
        "      title.append(\"\")\n",
        "    if soups.find(\"time\", {\"class\":\"entry-date updated td-module-date\"})!= None:\n",
        "      time.append(soups.find(\"time\", {\"class\":\"entry-date updated td-module-date\"}).text.replace(\"\\t\",\"\").replace(\"\\n\",\"\"))\n",
        "    else:\n",
        "      time.append(\"\")\n",
        "  \n",
        "    if soups.find(\"div\", {\"class\":\"td-post-content\"})!=None:\n",
        "      result = soups.find(\"div\", {\"class\":\"td-post-content\"}).findAll('p')\n",
        "      txtstring=\"\"\n",
        "      for x in result:\n",
        "      #print (x.text)\n",
        "        txtstring+=x.text.replace(u'\\xa0', u' ').replace('\\n',\" \")+\" \\n\"\n",
        "      texts.append(txtstring.strip())\n",
        "    else:\n",
        "      texts.append(\"\") \n",
        "\n",
        "    progresse = (cnt/len(soupx) * 100) \n",
        "    if progresse  % 10 == 0:\n",
        "      print (\"Scrapped \", progresse, \"% of the articles\")\n",
        "\n",
        "  return title,time,texts"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vTIoFziZFVst",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "5cd30715-e862-4f69-d2d3-5e3a1fd8eb63"
      },
      "source": [
        "cat_len = [get_no_featuredpages(cat) for cat in category]\n",
        "print(\"Got the following size for each category = \", cat_len)\n",
        "urls = set([scrap_link(category[i], cat_len[i]) for i in range(len(category))][0])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Got the following size for each category =  [916, 386, 39, 4]\n",
            "Getting the URL from the category :  ezoyolo  (featured) with  39  pages . \n",
            "Getting the URL from the category :  izimvo  (featured) with  4  pages . \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IcGn7ePbBlXW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "outputId": "79c06c2b-6de3-4998-ad8c-603e5ec3e3e8"
      },
      "source": [
        "soups = getSoup(list(urls))\n",
        "title,time,texts = getcontent(soups)\n",
        "#create a dictionarty\n",
        "import pandas as pd\n",
        "d = {'Date':time,'Title':title,'Text':texts}\n",
        "df = pd.DataFrame(d)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "getting soup objects for all the links ... \n",
            "Got  50.0 % of soup objects\n",
            "Got  100.0 % of soup objects\n",
            "got the soup object for all the links ... \n",
            "Scrapped  50.0 % of the articles\n",
            "Scrapped  100.0 % of the articles\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yDwuzEGvQxEp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "outputId": "d51642b4-d7f4-4c37-82c8-fcf4e40272b4"
      },
      "source": [
        "print(df)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date</th>\n",
              "      <th>Title</th>\n",
              "      <th>Text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Jul 20, 2016</td>\n",
              "      <td>Abadlali abane bayayishiya i-7de Laan!</td>\n",
              "      <td>ABADLALI bomdlalo we-Afrikaans kwiSABC 2 i-7de...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Apr 18, 2016</td>\n",
              "      <td>Kumabonakude wakho kule veki</td>\n",
              "      <td>Isidingo \\nULincoln uyama ngoSkhumbuzo ukuze a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Apr 2, 2020</td>\n",
              "      <td>Kumabonakude wakho kule veki</td>\n",
              "      <td>UThulani noGodfather bayaxabana.  \\nUSbu unesi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Apr 12, 2016</td>\n",
              "      <td>Uzibeka phantsi umdlali we-7de Laan!</td>\n",
              "      <td>KUPHINDE kwabhengezwa ukuba kukho omnye oyishi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Jul 5, 2016</td>\n",
              "      <td>Bonwabele ukondliwa yibhrakfesi kaDJ Sbu!</td>\n",
              "      <td>Ngokwenene isidlo seBhrakfesi sisidlo sokuqala...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>389</th>\n",
              "      <td>Nov 9, 2015</td>\n",
              "      <td>Kumabonakude wakho kule veki</td>\n",
              "      <td>Rhythm City \\nUThemba wenza konke okusemandlen...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>390</th>\n",
              "      <td>Apr 15, 2016</td>\n",
              "      <td>Isolezwe Gig Guideno-Agent Lungaz</td>\n",
              "      <td>Lwesihlanu, Tshazimpuzi 15 \\n*   Wise Guys (Vi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>391</th>\n",
              "      <td>Jan 25, 2016</td>\n",
              "      <td>Ubumenye-menye beekhamera kwilokishi  yaseDimb...</td>\n",
              "      <td>ISIKHULULO sikamabonakude iBay TV sifak’isandl...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>392</th>\n",
              "      <td>Apr 26, 2018</td>\n",
              "      <td>UTete ugxeleshe itumente enemali entle</td>\n",
              "      <td>Umakhwekhwetha wembethimanqindi yaseMdantsane,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>393</th>\n",
              "      <td>Jan 8, 2016</td>\n",
              "      <td>UJanet uthi akanamhlaza</td>\n",
              "      <td>Imvumi eyayiziwayo kwihlabathi uJanet Jackson ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>394 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "             Date  ...                                               Text\n",
              "0    Jul 20, 2016  ...  ABADLALI bomdlalo we-Afrikaans kwiSABC 2 i-7de...\n",
              "1    Apr 18, 2016  ...  Isidingo \\nULincoln uyama ngoSkhumbuzo ukuze a...\n",
              "2     Apr 2, 2020  ...  UThulani noGodfather bayaxabana.  \\nUSbu unesi...\n",
              "3    Apr 12, 2016  ...  KUPHINDE kwabhengezwa ukuba kukho omnye oyishi...\n",
              "4     Jul 5, 2016  ...  Ngokwenene isidlo seBhrakfesi sisidlo sokuqala...\n",
              "..            ...  ...                                                ...\n",
              "389   Nov 9, 2015  ...  Rhythm City \\nUThemba wenza konke okusemandlen...\n",
              "390  Apr 15, 2016  ...  Lwesihlanu, Tshazimpuzi 15 \\n*   Wise Guys (Vi...\n",
              "391  Jan 25, 2016  ...  ISIKHULULO sikamabonakude iBay TV sifak’isandl...\n",
              "392  Apr 26, 2018  ...  Umakhwekhwetha wembethimanqindi yaseMdantsane,...\n",
              "393   Jan 8, 2016  ...  Imvumi eyayiziwayo kwihlabathi uJanet Jackson ...\n",
              "\n",
              "[394 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yamPjv9C7ku-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#write the dataframe to file\n",
        "df.to_csv(r'xhosa_news.csv')"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3x0b4GvuJxk8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}